{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReQ_0EZI2UDR"
      },
      "source": [
        "#Chatbot RNN + Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb47J37kZS-4"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss9JD47u1VWg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "import itertools\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBld9HqAICil",
        "outputId": "4a9a5457-c358-400f-c6ad-a8a25923c150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy6LpS842_70"
      },
      "outputs": [],
      "source": [
        "CUDA= torch.cuda.is_available()\n",
        "device=torch.device(\"cuda\" if CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mntrnWX3lgL"
      },
      "source": [
        "##Load and Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2pkk34a3N5g"
      },
      "outputs": [],
      "source": [
        "base='/content/drive/MyDrive/Research/Chatbot_GRU/'\n",
        "os.chdir(base)\n",
        "##Dialog File Train\n",
        "dialog_file=os.path.join(base,'dialogues_train.txt')\n",
        "corpus_name='daily_dialog_corpus'\n",
        "# formatted_dialogs_file=os.path.join(base,'formatted_dialogs.txt')\n",
        "formatted_dialogs_file=os.path.join(base,'formatted_hindi_train.txt')\n",
        "##Dialog File Test\n",
        "dialog_file_test=os.path.join(base,'dialogues_test.txt')\n",
        "formatted_dialogs_file_test=os.path.join(base,'formatted_dialogs_test1.txt')\n",
        "delimiter='\\t'\n",
        "##########\n",
        "delimiter=str(codecs.decode(delimiter,'unicode_escape'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_iq1JEY38Fe"
      },
      "outputs": [],
      "source": [
        "def loadConversation(file):\n",
        "  lines=[]\n",
        "  with open(file,'r',encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      lines.append(line.strip().split(' __eou__ '))\n",
        "  return lines\n",
        "\n",
        "def extractSentencePairs(lines):\n",
        "  qa_pairs=[]\n",
        "  for conversation in lines:\n",
        "    for i in range(len(conversation)-1):\n",
        "      inputLine=conversation[i].strip().replace(' __eou__','')\n",
        "      targetLine=conversation[i+1].strip().replace(' __eou__','')\n",
        "      if inputLine and targetLine:\n",
        "        qa_pairs.append([inputLine,targetLine])\n",
        "  return qa_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsaU_ciO6pfw",
        "outputId": "a2f9fe2c-fb21-4723-b125-5805d8b2c8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing newly formatted file...\n"
          ]
        }
      ],
      "source": [
        "##Load Train Files and created formatted file\n",
        "lines=loadConversation(dialog_file)\n",
        "print('Writing newly formatted file...')\n",
        "with open(formatted_dialogs_file,'w',encoding='utf-8') as op:\n",
        "  writer=csv.writer(op,delimiter=delimiter,lineterminator='\\n')\n",
        "  for pair in extractSentencePairs(lines):\n",
        "    writer.writerow(pair)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n Sample Train Lines')\n",
        "with open(formatted_dialogs_file,'r') as f:\n",
        "  lines=f.readlines()\n",
        "\n",
        "for line in lines[35:40]:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeQJPhTMdJNy",
        "outputId": "39aa9d7e-1b9a-4c00-9e01-9263d3707937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sample Train Lines\n",
            "That ’ s bogus . I don't feel any stress at work , and my love life is practically nonexistent . This zodiac stuff is all a bunch of nonsense .\tNo , it ’ s not , your astrology sign can tell you a lot about your personality . See ? It says that an Aries is energetic and loves to socialize .\n",
            "\n",
            "No , it ’ s not , your astrology sign can tell you a lot about your personality . See ? It says that an Aries is energetic and loves to socialize .\tWell , you certainly match those criteria , but they ’ re so broad they could apply to anyone . What does it say about me ?\n",
            "\n",
            "Well , you certainly match those criteria , but they ’ re so broad they could apply to anyone . What does it say about me ?\tA Capricorn is serious-minded and practical . She likes to do things in conventional ways . That sounds just like you !\n",
            "\n",
            "Frank ’ s getting married , do you believe this ?\tIs he really ?\n",
            "\n",
            "Is he really ?\tYes , he is . He loves the girl very much .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Load Test Files and created formatted test file\n",
        "test_lines=loadConversation(dialog_file_test)\n",
        "print('Writing newly formatted test file...')\n",
        "with open(formatted_dialogs_file_test,'w',encoding='utf-8') as op:\n",
        "  writer=csv.writer(op,delimiter=delimiter,lineterminator='\\n')\n",
        "  for pair in extractSentencePairs(test_lines):\n",
        "    writer.writerow(pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC_xpR2c9Xj",
        "outputId": "6bb51795-c77a-477e-8432-c792fc7af5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing newly formatted test file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sg9408E9TLJ",
        "outputId": "b825f2bb-751e-4112-ae22-c343494a6826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sample test Lines\n",
            "b'Hey man , you wanna buy some weed ?\\tSome what ?\\n'\n",
            "b'Some what ?\\tWeed ! You know ? Pot , Ganja , Mary Jane some chronic !\\n'\n",
            "b'Weed ! You know ? Pot , Ganja , Mary Jane some chronic !\\tOh , umm , no thanks .\\n'\n",
            "b'Oh , umm , no thanks .\\tI also have blow if you prefer to do a few lines .\\n'\n",
            "b'I also have blow if you prefer to do a few lines .\\tNo , I am ok , really .\\n'\n"
          ]
        }
      ],
      "source": [
        "print('\\n Sample test Lines')\n",
        "with open(formatted_dialogs_file_test,'rb') as f:\n",
        "  lines=f.readlines()\n",
        "\n",
        "for line in lines[:5]:\n",
        "  print(line)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-jnVPLglSun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Hindi Data"
      ],
      "metadata": {
        "id": "47gTHH7zXS0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base='/content/drive/MyDrive/Research/Chatbot_GRU/'\n",
        "os.chdir(base)\n",
        "##Dialog File Train\n",
        "corpus_name='daily_dialog_corpus_hindi'\n",
        "train_file=os.path.join(base,'formatted_hindi_train_merged.txt')\n",
        "test_file=os.path.join(base,'formatted_hindi_test.txt')"
      ],
      "metadata": {
        "id": "fHzNns3TXWxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n Sample Train Lines')\n",
        "with open(train_file,'r') as f:\n",
        "  lines=f.readlines()\n",
        "\n",
        "for line in lines[35:40]:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9B68DUezCfo",
        "outputId": "f31396bd-81c2-4fe5-b72f-f816eeb25dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sample Train Lines\n",
            "यह फर्जी है। मुझे काम पर कोई तनाव महसूस नहीं होता है, और मेरा प्रेम जीवन व्यावहारिक रूप से अस्तित्वहीन है। यह राशि की सारी बातें बकवास हैं।\tनहीं, ऐसा नहीं है, आपका ज्योतिष संकेत आपको आपके व्यक्तित्व के बारे में बहुत कुछ बता सकता है। देखिए? इसमें कहा गया है कि मेष ऊर्जावान होता है और उसे सामाजिक जीवन पसंद होता है।\n",
            "\n",
            "नहीं, ऐसा नहीं है, आपका ज्योतिष संकेत आपको आपके व्यक्तित्व के बारे में बहुत कुछ बता सकता है। देखिए? इसमें कहा गया है कि मेष ऊर्जावान होता है और उसे सामाजिक जीवन पसंद होता है।\tखैर, आप निश्चित रूप से उन मानदंडों से मेल खाते हैं, लेकिन वे इतने व्यापक हैं कि वे किसी पर भी लागू हो सकते हैं। यह मेरे बारे में क्या कहता है?\n",
            "\n",
            "खैर, आप निश्चित रूप से उन मानदंडों से मेल खाते हैं, लेकिन वे इतने व्यापक हैं कि वे किसी पर भी लागू हो सकते हैं। यह मेरे बारे में क्या कहता है?\tएक मकर राशि गंभीर और व्यावहारिक होती है। उसे पारंपरिक तरीकों से काम करना पसंद है। यह बिल्कुल आपकी तरह लगता है।\n",
            "\n",
            "फ्रैंक की शादी हो रही है, क्या आप इस पर विश्वास करते हैं?\tक्या वह वास्तव में है?\n",
            "\n",
            "क्या वह वास्तव में है?\tवह उस लड़की से बहुत प्यार करता है।\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n Sample test Lines')\n",
        "with open(test_file,'r') as f:\n",
        "  lines=f.readlines()\n",
        "\n",
        "for line in lines[35:40]:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAMCZYAvzI0_",
        "outputId": "68843c0b-1c5d-48c1-cdba-a97f3639c1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sample test Lines\n",
            "क्या आप एक नेता हैं या अनुयायी?\tमैं लोगों का नेतृत्व करने की कोशिश नहीं करता। मैं इसके बजाय सभी के साथ सहयोग करूंगा और मिलकर काम करके काम पूरा करूंगा।\n",
            "\n",
            "मैं लोगों का नेतृत्व करने की कोशिश नहीं करता। मैं इसके बजाय सभी के साथ सहयोग करूंगा और मिलकर काम करके काम पूरा करूंगा।\tक्या आप सोचते हैं कि आप अपने आपको अंग्रेजी में आसानी से समझ सकते हैं?\n",
            "\n",
            "क्या आप सोचते हैं कि आप अपने आपको अंग्रेजी में आसानी से समझ सकते हैं?\tहां, ज्यादातर परिस्थितियों में।\n",
            "\n",
            "हां, ज्यादातर परिस्थितियों में।\tक्या आप यात्रा के लिए उपलब्ध हैं?\n",
            "\n",
            "क्या आप यात्रा के लिए उपलब्ध हैं?\tहां, मुझे यात्रा करना पसंद है। मैं युवा और अविवाहित हूं। मुझे अक्सर यात्रा करने में कोई समस्या नहीं है।\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suCcZ9PtF98Z"
      },
      "source": [
        "## Load And Trim Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5w6JPNQGA-R"
      },
      "outputs": [],
      "source": [
        "PAD_tok=0\n",
        "SOS_tok=1\n",
        "EOS_tok=2\n",
        "OOV_tok=3\n",
        "\n",
        "class Vocabulary:\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "    self.word2idx={}\n",
        "    self.word2count={}\n",
        "    self.idx2word={PAD_tok:'PAD',SOS_tok:'SOS',EOS_tok:'EOS',OOV_tok:'OOV'}\n",
        "    self.num_words=4\n",
        "    self.word2idx['OOV']=3\n",
        "  \n",
        "  def addLine(self,line):\n",
        "    for word in line.split(' '):\n",
        "      self.addWord(word)\n",
        "  def addWord(self,word):\n",
        "    if word not in self.word2idx:\n",
        "      self.word2idx[word]=self.num_words\n",
        "      self.word2count[word]=1\n",
        "      self.idx2word[self.num_words]=word\n",
        "      self.num_words+=1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17HdNdt5KXfV"
      },
      "outputs": [],
      "source": [
        "def unicodeToASCII(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def normalizeString(text):\n",
        "  # s = unicodeToASCII(s.lower().strip())\n",
        "   text = text.lower()\n",
        "   text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text)\n",
        "   text = re.sub('@[^s]+','',text)\n",
        "   text = re.sub('[s]+', ' ', text)\n",
        "   text = re.sub(r'#([^s]+)', r'1', text)\n",
        "   text = re.sub('[.!:?-]', '', text)\n",
        "   text = re.sub('[a-zA-Z0-9]','',text)\n",
        "   text = re.sub(' +', ' ',text)\n",
        "   text = text.strip('\"\"')\n",
        "   return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy06y3I9ONy4",
        "outputId": "92037d4e-7a10-4719-c88e-1e0b7911ed13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading File please wait...\n",
            "Done Reading.....\n"
          ]
        }
      ],
      "source": [
        "print('Reading File please wait...')\n",
        "lines=open(train_file,).read().strip().split('\\n')\n",
        "pairs_data=[[normalizeString(s) for s in pair.split('\\t')] for pair in lines]\n",
        "print('Done Reading.....')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsWDa_WLSv27",
        "outputId": "7eb95a6b-5595-4ccb-b9d6-fc8aa381cb12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 76053 pairs in the dataset\n",
            "After filtering, there are 54060 pair\n"
          ]
        }
      ],
      "source": [
        "MAX_LENGTH=20\n",
        "# Load/Assemble voc and pairs\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "def filterPair(p):\n",
        "  return len(p[0].split())<MAX_LENGTH and len(p[1].split())<MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "pairs=pairs_data\n",
        "pairs=[pair for pair in pairs if len(pair)>1]\n",
        "print(\"There are {} pairs in the dataset\".format(len(pairs)))\n",
        "pairs=filterPairs(pairs)\n",
        "print('After filtering, there are {} pair'.format(len(pairs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlNqfkCvVFhe"
      },
      "source": [
        "##Creating Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-efxPCITuHg",
        "outputId": "2367b40f-8dde-44ad-fe2b-a741584ec71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Vocabulary....\n",
            "counted words: 16172\n"
          ]
        }
      ],
      "source": [
        "print('Creating Vocabulary....')\n",
        "vocab=Vocabulary(corpus_name)\n",
        "\n",
        "for pair in pairs:\n",
        "  vocab.addLine(pair[0])\n",
        "  vocab.addLine(pair[1])\n",
        "print(\"counted words:\",vocab.num_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.idx2word[510]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Yfh9uhGfYQfh",
        "outputId": "877d2408-ef81-4a70-b9c7-0fe5fb97282d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'लंबा'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP4427-XZNZO"
      },
      "source": [
        "##Prepare the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiF0VCHbZPT-"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(vocab, sentence):\n",
        "    return [vocab.word2idx[word] if word in vocab.word2idx.keys() else vocab.word2idx['OOV'] for word in sentence.split(' ')] + [EOS_tok]\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_tok):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_tok):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_tok:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# Returns all items for a given batch of pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFXvA9I1vKH1"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ1iCatI004H"
      },
      "source": [
        "###1.Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYSMZd4WvQoP"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self,hidden_size,embedding,n_layers=1,dropout=0):\n",
        "    super(EncoderRNN,self).__init__()\n",
        "    self.n_layer=n_layers\n",
        "    self.hidden_size=hidden_size\n",
        "    self.embedding=embedding\n",
        "    self.gru=nn.GRU(hidden_size,hidden_size,n_layers,dropout=dropout,bidirectional=True)\n",
        "\n",
        "  def forward(self,input_seq,input_length,hidden=None):\n",
        "    embedded=self.embedding(input_seq)\n",
        "    packed=torch.nn.utils.rnn.pack_padded_sequence(embedded,input_length)\n",
        "    output,hidden=self.gru(packed,hidden)\n",
        "    output,_=torch.nn.utils.rnn.pad_packed_sequence(output)\n",
        "    output=output[:,:,:self.hidden_size]+output[:,:,self.hidden_size:]\n",
        "    return output,hidden\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqZXHN7Y0tnu"
      },
      "source": [
        "###2.Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42rYOxhi0xwM"
      },
      "outputs": [],
      "source": [
        "# Luong attention layer\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd3tNSuo48mw"
      },
      "outputs": [],
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRNW5rb0YQfJ"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH3U1KQdYSNQ"
      },
      "outputs": [],
      "source": [
        "def maskNLLLoss(decoder_out,target,mask):\n",
        "  nTotal=mask.sum()\n",
        "  target=target.view(-1,1)\n",
        "  gathered_tensor=torch.gather(decoder_out,1,target)\n",
        "  crossEntropy= -torch.log(gathered_tensor)\n",
        "  loss=crossEntropy.masked_select(mask)\n",
        "  loss=loss.mean()\n",
        "  loss=loss.to(device)\n",
        "  return loss,nTotal.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJLcYn14Z1hK"
      },
      "outputs": [],
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for rnn packing should always be on the cpu\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_tok for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    # use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    use_teacher_forcing=False\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o8c-c2Vgm9K"
      },
      "outputs": [],
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGdXMDuUiFxg"
      },
      "source": [
        "###Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fnp-rcUZiIku",
        "outputId": "7b73ed89-7d3f-4ea8-a92a-100103f1d954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ],
      "source": [
        "# Configure models\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#attn_model = 'general'\n",
        "#attn_model = 'concat'\n",
        "hidden_size = 512\n",
        "encoder_n_layers = 4\n",
        "decoder_n_layers = 4\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    vocab.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(vocab.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bk78jeAg0ie"
      },
      "outputs": [],
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have cuda, configure cuda to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BueZoCWbg7nP"
      },
      "source": [
        "##Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqnm3sVog80f"
      },
      "outputs": [],
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_tok\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTkI-3HihJwu"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.idx2word[token.item()] for token in tokens]\n",
        "    return decoded_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjbTvaboxQsH"
      },
      "source": [
        "##Chat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "metadata": {
        "id": "NjmJLN7Mg9pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOMRkWKmhN3w"
      },
      "outputs": [],
      "source": [
        "# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting (uncomment and run the following line to begin)\n",
        "evaluateInput(encoder, decoder, searcher, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test the model"
      ],
      "metadata": {
        "id": "PkeRwUUAaAjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation Metrics"
      ],
      "metadata": {
        "id": "3Ref6QCzhSCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Bleu Score\n",
        "def bleu_score(guess, answer):\n",
        "    \"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"\n",
        "    bleu1=sentence_bleu([normalizeString(answer).split()],normalizeString(guess).split(\" \"),weights=[1,0,0,0])\n",
        "    bleu2=sentence_bleu([normalizeString(answer).split()],normalizeString(guess).split(\" \"),weights=[0.5,0.5,0,0])\n",
        "    bleu3=sentence_bleu([normalizeString(answer).split()],normalizeString(guess).split(\" \"),weights=[0.3,0.3,0.3,0])\n",
        "    return [bleu1,bleu2,bleu3]"
      ],
      "metadata": {
        "id": "h_c0lP-EWpJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##f1_score\n",
        "def prec_recall_f1_score(pred_items, gold_items)->float:\n",
        "    common = Counter(gold_items) & Counter(pred_items)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_items)\n",
        "    recall = 1.0 * num_same / len(gold_items)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def f1_score(guess, answer)-> float:\n",
        "    \"\"\"Return the max F1 score between the guess and *any* answer.\"\"\"\n",
        "    if guess is None or answer is None:\n",
        "        return 0\n",
        "    g_tokens = normalizeString(guess).split()\n",
        "    a_token=normalizeString(answer).split()\n",
        "    return prec_recall_f1_score(g_tokens,a_token )"
      ],
      "metadata": {
        "id": "xWcmPy56_A_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Meteor Score\n",
        "def _meteor_score(guess,answer):\n",
        "  return meteor_score([normalizeString(answer).split()],normalizeString(guess).split())"
      ],
      "metadata": {
        "id": "QTDySMdEBmzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateMetrics(test_data):\n",
        "  num_samples=len(test_data)\n",
        "  f1=0.0\n",
        "  bleu=[0.0,0.0,0.0]\n",
        "  meteor=0.0\n",
        "  for pair in test_data:\n",
        "    pred = evaluate(encoder, decoder, searcher, vocab, pair[0])\n",
        "    pred[:] = [x for x in pred if not (x == 'EOS' or x == 'PAD')]\n",
        "    pred=' '.join(pred)\n",
        "    actual=pair[1]\n",
        "    f1+=f1_score(pred,actual)\n",
        "    bleu=[a+b for a,b in zip(bleu,bleu_score(pred,actual))]\n",
        "    meteor+=_meteor_score(pred,actual)\n",
        "  f1=f1/num_samples\n",
        "  bleu[:]=[x/num_samples for x in bleu]\n",
        "  meteor=meteor/num_samples\n",
        "  print(f'F1_score: {f1:.2f}')\n",
        "  print(f'BLEU-1_score: {bleu[0]:.3f}')\n",
        "  print(f'BLEU-2_score: {bleu[1]:.3f}')\n",
        "  print(f'BLEU-3_score: {bleu[2]:.2f}')\n",
        "  print(f'meteor_score: {meteor:.2f}')\n",
        "  return f1,bleu,meteor"
      ],
      "metadata": {
        "id": "QnJLI8k-Xw5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reading lines from formatted test files please wait...')\n",
        "test_lines=open(test_file,encoding='utf-8').read().strip().split('\\n')\n",
        "pairs_test_data=[[normalizeString(s) for s in pair.split('\\t')] for pair in test_lines]\n",
        "print('Done Reading.....')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l46UlgapF1tX",
        "outputId": "8fe664dd-e342-4086-8ef2-46e795f8ef24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines from formatted test files please wait...\n",
            "Done Reading.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1,bleu,meteor=evaluateMetrics(pairs_test_data)"
      ],
      "metadata": {
        "id": "1PMyByHc3lVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G-YijemQlPtH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}