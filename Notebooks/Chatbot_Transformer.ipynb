{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Chatbot Transformer"
      ],
      "metadata": {
        "id": "HnQPa1xYUMQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "zQszjOGgUL6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leuh-vUKPvzx",
        "outputId": "e1c86394-bb33-4cb4-c941-4f4abcdda6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT2hxulLSxsV"
      },
      "outputs": [],
      "source": [
        "CUDA= torch.cuda.is_available()\n",
        "device=torch.device(\"cuda\" if CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base='/content/drive/MyDrive/Research/Chatbot_GRU/'\n",
        "os.chdir(base)\n",
        "corpus_name='Daily_Dialog_Hindi'\n",
        "formatted_dialogs_file=os.path.join(base,'formatted_hindi_train_merged.txt')\n",
        "formatted_val_file=os.path.join(base,'formatted_hindi_val.txt')\n",
        "formatted_test_file=os.path.join(base,'formatted_hindi_test.txt')"
      ],
      "metadata": {
        "id": "d6XVm2s_VUzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext='/content/drive/MyDrive/Chatbot_research/fastText'\n",
        "os.chdir(fasttext)\n",
        "!pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwDDC-h8c2w9",
        "outputId": "770cc459-7ed2-4dfc-9da3-0647c25cb09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/drive/.shortcut-targets-by-id/1T6l4xRdCXp4XqFlU2jinSImxAHDOS9be/Chatbot_research/fastText\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from fasttext==0.9.2) (67.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fasttext==0.9.2) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=4381862 sha256=721e7628e25ef89a3381e332d51caf34a7eb4ff482b2a4be98b71fcfac7cabac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dhcn01z3/wheels/40/3d/d1/b95cc40292178b04d2e652bc831a3357b5812c4455f530c04e\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFOHncmPguww",
        "outputId": "df9c4533-fc5b-480d-8238-fd506388e7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1T6l4xRdCXp4XqFlU2jinSImxAHDOS9be/Chatbot_research\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "ft_model = fasttext.load_model('cc.hi.300.bin')"
      ],
      "metadata": {
        "id": "YPEh4MA8dXdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(base)"
      ],
      "metadata": {
        "id": "ZNylJ2wydYMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "_e7qhOexbSsu",
        "outputId": "02ff2dd3-ae95-4522-c54f-0ea19e13f4ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot_GRU.ipynb\t\t  formatted_hindi_val.txt\n",
            "Chatbot_Transformer.ipynb\t  pairs_encoded_train.json\n",
            "en-indic.zip\t\t\t  pairs_encoded_val.json\n",
            "formatted_hindi_test.txt\t  Preprocessing.ipynb\n",
            "formatted_hindi_train_merged.txt  results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_tok=0\n",
        "SOS_tok=1\n",
        "EOS_tok=2\n",
        "OOV_tok=3\n",
        "\n",
        "class Vocabulary:\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "    self.word2idx={'PAD':PAD_tok,'SOS':SOS_tok,'EOS':EOS_tok,'OOV':OOV_tok}\n",
        "    self.word2count={}\n",
        "    self.idx2word={PAD_tok:'PAD',SOS_tok:'SOS',EOS_tok:'EOS',OOV_tok:'OOV'}\n",
        "    self.num_words=4\n",
        "  \n",
        "  def addLine(self,line):\n",
        "    for word in line.split(' '):\n",
        "      self.addWord(word)\n",
        "  def addWord(self,word):\n",
        "    if word not in self.word2idx:\n",
        "      self.word2idx[word]=self.num_words\n",
        "      self.word2count[word]=1\n",
        "      self.idx2word[self.num_words]=word\n",
        "      self.num_words+=1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "6fQ1hmBXVpYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeString(text):\n",
        "  # s = unicodeToASCII(s.lower().strip())\n",
        "   text = text.lower()\n",
        "   text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text)\n",
        "   text = re.sub('@[^s]+','',text)\n",
        "   text = re.sub('[s]+', ' ', text)\n",
        "   text = re.sub(r'#([^s]+)', r'1', text)\n",
        "   text = re.sub('[.!:?-]', '', text)\n",
        "   text = re.sub('[a-zA-Z0-9]','',text)\n",
        "   text = re.sub(' +', ' ',text)\n",
        "   text = text.strip('\"\"')\n",
        "   return text"
      ],
      "metadata": {
        "id": "dKEkP4gTV2Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reading Train File please wait...')\n",
        "lines=open(formatted_dialogs_file,).read().strip().split('\\n')\n",
        "pairs_data_train=[[normalizeString(s) for s in pair.split('\\t')] for pair in lines]\n",
        "print('Done Reading.....')"
      ],
      "metadata": {
        "id": "o3R9AB8LWCnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e5b1de-ae67-4207-d602-6f7e7ce99fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Train File please wait...\n",
            "Done Reading.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reading validation File please wait...')\n",
        "lines=open(formatted_val_file).read().strip().split('\\n')\n",
        "pairs_data_val=[[normalizeString(s) for s in pair.split('\\t')] for pair in lines]\n",
        "print('Done Reading.....')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCPCuAsLt9Ki",
        "outputId": "1c4d5dd3-9f67-4f2a-9337-8e17fad01ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading validation File please wait...\n",
            "Done Reading.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reading test File please wait...')\n",
        "lines=open(formatted_test_file).read().strip().split('\\n')\n",
        "pairs_data_test=[[normalizeString(s) for s in pair.split('\\t')] for pair in lines]\n",
        "print('Done Reading.....')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HU2xnPyuLT-",
        "outputId": "78cf6fae-241b-4a2b-f2f7-105a45f26a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading test File please wait...\n",
            "Done Reading.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH=16\n",
        "# Load/Assemble voc and pairs\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "def filterPair(p):\n",
        "  return len(p[0].split())<MAX_LENGTH and len(p[1].split())<MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "-DZIxC1fWITT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_train=[pair for pair in pairs_data_train if len(pair)>1]\n",
        "print(\"There are {} pairs in the train dataset\".format(len(pairs_train)))\n",
        "pairs_train=filterPairs(pairs_train)\n",
        "print('After filtering, there are {} pair'.format(len(pairs_train)))\n",
        "\n",
        "pairs_val=[pair for pair in pairs_data_val if len(pair)>1]\n",
        "print(\"There are {} pairs in the val dataset\".format(len(pairs_val)))\n",
        "pairs_val=filterPairs(pairs_val)\n",
        "print('After filtering, there are {} pair'.format(len(pairs_val)))\n",
        "\n",
        "pairs_test=[pair for pair in pairs_data_test if len(pair)>1]\n",
        "print(\"There are {} pairs in the test dataset\".format(len(pairs_test)))\n",
        "pairs_test=filterPairs(pairs_test)\n",
        "print('After filtering, there are {} pair'.format(len(pairs_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcmTrrjnvEhq",
        "outputId": "e53302eb-5e00-4583-f7ff-f829ccb88473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 76053 pairs in the train dataset\n",
            "After filtering, there are 43534 pair\n",
            "There are 7069 pairs in the val dataset\n",
            "After filtering, there are 4076 pair\n",
            "There are 6740 pairs in the test dataset\n",
            "After filtering, there are 3814 pair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Creating Vocabulary....')\n",
        "vocab=Vocabulary(corpus_name)\n",
        "\n",
        "for pair in pairs_train:\n",
        "  vocab.addLine(pair[0])\n",
        "  vocab.addLine(pair[1])\n",
        "print(\"counted words:\",vocab.num_words)"
      ],
      "metadata": {
        "id": "t3LOCyeUWNoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbfbfbf-8dba-43e1-ed8f-32775653a6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Vocabulary....\n",
            "counted words: 13906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim=300\n",
        "embedding_matrix = np.zeros((vocab.num_words, emb_dim))\n",
        "\n",
        "for i, word in enumerate(vocab.word2idx):\n",
        "    if word in ft_model:\n",
        "        embedding_matrix[i] = ft_model[word]\n",
        "    else:\n",
        "        embedding_matrix[i] = np.random.randn(emb_dim)"
      ],
      "metadata": {
        "id": "pQ74bcoQdiKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_question(sentence,vocab):\n",
        "  enc=[vocab.word2idx.get(word,vocab.word2idx['OOV']) for word in sentence.split()] + [vocab.word2idx['PAD']]*(MAX_LENGTH- len(sentence.split()))\n",
        "  return enc\n",
        "\n",
        "def encode_reply(sentence,vocab):\n",
        "  enc=[vocab.word2idx['SOS']]+[vocab.word2idx.get(word,vocab.word2idx['OOV'])  for word in sentence.split()] +[vocab.word2idx['EOS']]+ [vocab.word2idx['PAD']]*(MAX_LENGTH- len(sentence.split()))\n",
        "  return enc\n",
        "  "
      ],
      "metadata": {
        "id": "3bcPv58wWWDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_encoded_train=[]\n",
        "for pair in pairs_train:\n",
        "  ques=encode_question(pair[0],vocab)\n",
        "  reply=encode_reply(pair[1],vocab)\n",
        "  pairs_encoded_train.append([ques,reply])\n",
        "\n",
        "with open('pairs_encoded_train.json','w') as fp1:\n",
        "  json.dump(pairs_encoded_train,fp1)\n",
        "\n",
        "pairs_encoded_val=[]\n",
        "for pair in pairs_val:\n",
        "  ques=encode_question(pair[0],vocab)\n",
        "  reply=encode_reply(pair[1],vocab)\n",
        "  pairs_encoded_val.append([ques,reply])\n",
        "\n",
        "with open('pairs_encoded_val.json','w') as fp2:\n",
        "  json.dump(pairs_encoded_val,fp2)"
      ],
      "metadata": {
        "id": "j_-NA2QfdNZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "  def __init__(self,filename):\n",
        "    self.pairs= json.load(open(filename))\n",
        "    self.dataset_size= len(self.pairs)\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    ques=torch.LongTensor(self.pairs[i][0])\n",
        "    reply=torch.LongTensor(self.pairs[i][1])\n",
        "    return ques,reply\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.dataset_size"
      ],
      "metadata": {
        "id": "PC_ya6qrgDa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader= DataLoader(Dataset(filename='pairs_encoded_train.json'),batch_size=64,shuffle=True,pin_memory=True)"
      ],
      "metadata": {
        "id": "iHnwtkkX5pka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loader=DataLoader(Dataset(filename='pairs_encoded_val.json'),batch_size=64,shuffle=True,pin_memory=True)"
      ],
      "metadata": {
        "id": "teboHr1inb8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(question, reply_input, reply_target):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = question!=0\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    reply_input_mask = reply_input!=0\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
        "    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n",
        "    \n",
        "    return question_mask, reply_input_mask, reply_target_mask\n"
      ],
      "metadata": {
        "id": "hFt2rrW76Oi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model,embedding_matrix, max_len = 17):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.embed = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding\n"
      ],
      "metadata": {
        "id": "dFkmSj9ekVo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted "
      ],
      "metadata": {
        "id": "9IeOO-b935g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "8flAz3fybiFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "ZcF4riLkc_ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "psU2Id_lfgth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads, num_layers, word_map,embedding_matrix):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = len(word_map)\n",
        "        self.embed = Embeddings(self.vocab_size, d_model,embedding_matrix)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
        "        \n",
        "    def encode(self, src_words, src_mask):\n",
        "        src_embeddings = self.embed(src_words)\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "        tgt_embeddings = self.embed(target_words)\n",
        "        for layer in self.decoder:\n",
        "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "        return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "_avenQuwjtwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamWarmup:\n",
        "  def __init__(self,model_size,warmup_steps,optimizer):\n",
        "    self.model_size=model_size\n",
        "    self.warmup_steps=warmup_steps\n",
        "    self.optimizer=optimizer\n",
        "    self.curr_step=0\n",
        "    self.lr=0\n",
        "\n",
        "  def get_lr(self):\n",
        "    return self.model_size**(-0.5)*min(self.curr_step**(-0.5),self.curr_step*self.warmup_steps**(-1.5))\n",
        "\n",
        "  def step(self):\n",
        "    self.curr_step+=1\n",
        "    lr=self.get_lr()\n",
        "    for param_group in self.optimizer.param_groups:\n",
        "      param_group['lr']=lr\n",
        "    self.lr=lr\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "wpRGkAqSkqPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossWithLS(nn.Module):\n",
        "  def __init__(self,size,smooth):\n",
        "    super(LossWithLS,self).__init__()\n",
        "    self.criterion= nn.KLDivLoss(size_average=False, reduce=False)\n",
        "    self.confidence=1-smooth\n",
        "    self.smooth=smooth\n",
        "    self.size=size\n",
        "\n",
        "  def forward(self,pred,tar,mask):\n",
        "    \"\"\"\n",
        "    pred=(batch_size,max_words,vocab_size)\n",
        "    tar,mask=(batch_size,max_words )\n",
        "    \"\"\"\n",
        "    pred=pred.view(-1,pred.size(-1))\n",
        "    tar=tar.contiguous().view(-1)\n",
        "    mask=mask.float()\n",
        "    mask=mask.view(-1)\n",
        "    labels=pred.data.clone()\n",
        "    labels.fill_(self.smooth/(self.size-1))\n",
        "    labels.scatter_(1,tar.data.unsqueeze(1),self.confidence)\n",
        "    loss= self.criterion(pred,labels)\n",
        "    loss=(loss.sum(1)*mask).sum()/mask.sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "a03lzHIPFLM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def transformer_loss(y_pred, y_true, mask, alpha=0.5):\n",
        "#     \"\"\"\n",
        "#     Computes a combination of Cross-Entropy Loss and Mean Squared Error (MSE) Loss for a Transformer.\n",
        "\n",
        "#     :param y_pred: predicted sequence, shape (batch_size, sequence_length, num_classes)\n",
        "#     :param y_true: true sequence, shape (batch_size, sequence_length)\n",
        "#     :param mask: masking tensor, shape (batch_size, sequence_length)\n",
        "#     :param alpha: weight for the MSE loss term\n",
        "#     :return: loss, a scalar tensor\n",
        "#     \"\"\"\n",
        "#     # print('y_pred shape:',y_pred.shape)\n",
        "#     # print('y_true shape:',y_true.shape)\n",
        "#     # print('mask shape:',mask.shape)\n",
        "#     # Compute the cross-entropy loss\n",
        "#     # ce_loss = torch.nn.CrossEntropyLoss(reduction='none')(y_pred.view(-1, y_pred.size(-1)), y_true.view(-1))\n",
        "#     ce_loss = torch.nn.CrossEntropyLoss(reduction='none')(y_pred.reshape(-1, y_pred.size(-1)), y_true.reshape(-1))\n",
        "#     # Apply the mask to the ce loss\n",
        "#     ce_loss = (ce_loss * mask.view(-1)).mean()\n",
        "#     # Compute the mean squared error loss\n",
        "#     # mse_loss = torch.nn.MSELoss()(y_pred.view(-1, y_pred.size(-1)), y_true.view(-1, y_pred.size(-1)))\n",
        "#     # mse_loss = torch.nn.MSELoss(reduction='none')(y_pred.reshape(-1, y_pred.size(-1)), y_true.reshape(-1,))\n",
        "#     # y_pred_flattened = y_pred.reshape(-1, y_pred.size(-1))\n",
        "#     # y_true_flattened = y_true.reshape(-1)\n",
        "#     # mse_loss = torch.nn.MSELoss()(y_pred_flattened[mask], y_true_flattened[mask])\n",
        "\n",
        "#     # # Compute the combined loss\n",
        "#     # loss = (1-alpha)*ce_loss + alpha*mse_loss\n",
        "#     return ce_loss"
      ],
      "metadata": {
        "id": "el9Y4I_DXqkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUWxUjZPyQEa",
        "outputId": "dde8135f-653e-427b-bd62-e5eadac70148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13906, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model=300\n",
        "heads=6\n",
        "num_layer=6\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model=Transformer(d_model,heads,num_layer,vocab.word2idx,embedding_matrix)\n",
        "model.embed.embed.requires_grad=False\n",
        "model.to(device)\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0,betas=(0.9,0.98),eps=1e-9)\n",
        "transformer_optimizer=AdamWarmup(model_size=d_model,warmup_steps=4000,optimizer=optimizer)\n",
        "criterion=LossWithLS(size=len(vocab.word2idx),smooth=0.1)"
      ],
      "metadata": {
        "id": "VeuSjHzLNi6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader,model,criterion,epoch):\n",
        "  model.train()\n",
        "  sum_loss=0\n",
        "  count=0\n",
        "  for i,(dialog,response) in enumerate(train_loader):\n",
        "    samples=dialog.shape[0]\n",
        "    dialog=dialog.to(device)\n",
        "    response=response.to(device)\n",
        "    ##Sentence: <sos> I am working at Google <eos>\n",
        "    # input=<sos> I    am    working    at       Google\n",
        "    #target=  I   am wroking    at     Google     <eos>\n",
        "    response_input= response[:,:-1]\n",
        "    response_target = response[:,1:]\n",
        "\n",
        "    dialog_mask,response_ip_mask,response_tar_mask=create_masks(dialog,response_input,response_target)\n",
        "\n",
        "    #Run through Model\n",
        "    output=model(src_words=dialog,src_mask=dialog_mask,target_words=response_input,target_mask=response_ip_mask)\n",
        "    loss=criterion(output,response_target,response_tar_mask)\n",
        "\n",
        "    #backpropagation\n",
        "    transformer_optimizer.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    transformer_optimizer.step()\n",
        "    sum_loss+=loss.item()*samples\n",
        "    count+=samples\n",
        "\n",
        "    # if i%100==0:\n",
        "    #   print(\"Epoch [{}] [{}/{}] \\t Train Loss:{:3f}\".format(epoch,i,len(train_loader),sum_loss/count))\n",
        "  return sum_loss/count"
      ],
      "metadata": {
        "id": "wSufB3HWQS8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(eval_loader,model,criterion,epoch):\n",
        "   with torch.no_grad():\n",
        "    model.eval()\n",
        "    sum_loss=0\n",
        "    count=0\n",
        "    for i,(dialog,response) in enumerate(eval_loader):\n",
        "      samples=dialog.shape[0]\n",
        "      dialog=dialog.to(device)\n",
        "      response=response.to(device)\n",
        "      response_input= response[:,:-1]\n",
        "      response_target = response[:,1:]\n",
        "\n",
        "      dialog_mask,response_ip_mask,response_tar_mask=create_masks(dialog,response_input,response_target)\n",
        "\n",
        "      #Run through Model\n",
        "      output=model(src_words=dialog,src_mask=dialog_mask,target_words=response_input,target_mask=response_ip_mask)\n",
        "\n",
        "      loss=criterion(output,response_target,response_tar_mask)\n",
        "      sum_loss+=loss.item()*samples\n",
        "      count+=samples\n",
        "\n",
        "      # if i%100==0:\n",
        "      #   print(\"Epoch [{}] [{}/{}] \\t Val Loss:{:3f}\".format(epoch,i,len(eval_loader),sum_loss/count))\n",
        "    return sum_loss/count"
      ],
      "metadata": {
        "id": "PszPDIXZjrNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateChat(model,dialog,dialog_mask,max_len,vocab):\n",
        "  rev_vocab=vocab.idx2word\n",
        "  model.eval()\n",
        "  start_token=vocab.word2idx['SOS']\n",
        "  encoded=model.encode(dialog,dialog_mask)\n",
        "  words=torch.LongTensor([[start_token]]).to(device)\n",
        "  for step in range(max_len-1):\n",
        "    size=words.shape[0]\n",
        "    target_mask= torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8)\n",
        "    target_mask=target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
        "    decoded = model.decode(words,target_mask,encoded,dialog_mask)\n",
        "    ## decoded is of shape (1,1,vocab_size)\n",
        "    prediction = model.logit(decoded[:,-1])\n",
        "    ## prediction is shape (1,vocab_size)\n",
        "    _,next_word=torch.max(prediction,dim=1)\n",
        "    next_word=next_word.item()\n",
        "    if next_word==vocab.word2idx['EOS']:\n",
        "      break\n",
        "    words=torch.cat([words,torch.LongTensor([[next_word]]).to(device)],dim=1)\n",
        "\n",
        "  words=words.squeeze(0)\n",
        "  words=words.tolist()\n",
        "  sen_idx=[w for w in words if w not in {vocab.word2idx['SOS']}]\n",
        "  sentence= \" \".join([rev_vocab[sen_idx[i]] for i in range(len(sen_idx))])\n",
        "\n",
        "  return sentence\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "StRZhck4aHl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test Code"
      ],
      "metadata": {
        "id": "vkPdh6b66aQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3r0NCBvmeby",
        "outputId": "62789648-a8b0-4dad-9ca5-3f5ff3494079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "def calculateRogue(test_data):\n",
        "  prediction=[]\n",
        "  actual=[]\n",
        "  for pair in test_data:\n",
        "    dialog=pair[0]\n",
        "    enc_dialog=[vocab.word2idx.get(word,vocab.word2idx['OOV']) for word in dialog.split()]\n",
        "    dialog=torch.LongTensor(enc_dialog).to(device).unsqueeze(0)\n",
        "    dialog_mask=(dialog!=0).to(device).unsqueeze(1).unsqueeze(1)\n",
        "    pred=evaluateChat(model,dialog,dialog_mask,MAX_LENGTH,vocab)\n",
        "    if len(pred)<=0:\n",
        "      continue\n",
        "    prediction.append(pred)\n",
        "    actual.append(pair[1])\n",
        "  rouge = Rouge()\n",
        "  scores = rouge.get_scores(prediction, actual,avg=True)\n",
        "  print(scores)\n",
        "  return scores\n",
        "\n",
        "##Bleu Score\n",
        "def bleu_score(guess, answer):\n",
        "    \"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"\n",
        "    bleu1=sentence_bleu([normalizeString(answer).split()],normalizeString(guess).split(\" \"),weights=[1,0,0,0])\n",
        "    bleu2=sentence_bleu([normalizeString(answer).split()],normalizeString(guess).split(\" \"),weights=[0.5,0.5,0,0])\n",
        "    bleu3=sentence_bleu([normalizeString(answer).split()],normalizeString(guess).split(\" \"),weights=[0.3,0.3,0.3,0])\n",
        "    return [bleu1,bleu2,bleu3]\n",
        "##f1_score\n",
        "def prec_recall_f1_score(pred_items, gold_items)->float:\n",
        "    common = Counter(gold_items) & Counter(pred_items)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_items)\n",
        "    recall = 1.0 * num_same / len(gold_items)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def f1_score(guess, answer)-> float:\n",
        "    \"\"\"Return the max F1 score between the guess and *any* answer.\"\"\"\n",
        "    if guess is None or answer is None:\n",
        "        return 0\n",
        "    g_tokens = normalizeString(guess).split()\n",
        "    a_token=normalizeString(answer).split()\n",
        "    return prec_recall_f1_score(g_tokens,a_token )\n",
        "\n",
        "##Meteor Score\n",
        "def _meteor_score(guess,answer):\n",
        "  return meteor_score([normalizeString(answer).split()],normalizeString(guess).split())\n",
        "\n",
        "def evaluateMetrics(test_data):\n",
        "  num_samples=len(test_data)\n",
        "  f1=0.0\n",
        "  bleu=[0.0,0.0,0.0]\n",
        "  meteor=0.0\n",
        "  for pair in test_data:\n",
        "    dialog=pair[0]\n",
        "    actual=pair[1]\n",
        "    enc_dialog=[vocab.word2idx.get(word,vocab.word2idx['OOV']) for word in dialog.split()]\n",
        "    dialog=torch.LongTensor(enc_dialog).to(device).unsqueeze(0)\n",
        "    dialog_mask=(dialog!=0).to(device).unsqueeze(1).unsqueeze(1)\n",
        "    pred=evaluateChat(model,dialog,dialog_mask,MAX_LENGTH,vocab)\n",
        "    f1+=f1_score(pred,actual)\n",
        "    bleu=[a+b for a,b in zip(bleu,bleu_score(pred,actual))]\n",
        "    meteor+=_meteor_score(pred,actual)\n",
        "  f1=(f1/num_samples)*100\n",
        "  bleu[:]=[(x/num_samples)*100 for x in bleu]\n",
        "  meteor=(meteor/num_samples)*100\n",
        "  print(f'F1_score: {f1:.2f}')\n",
        "  print(f'BLEU-1_score: {bleu[0]:.3f}')\n",
        "  print(f'BLEU-2_score: {bleu[1]:.3f}')\n",
        "  print(f'BLEU-3_score: {bleu[2]:.2f}')\n",
        "  print(f'meteor_score: {meteor:.2f}')\n",
        "  return f1,bleu,meteor"
      ],
      "metadata": {
        "id": "RJsUm19M6Zz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=40\n",
        "train_loss_list=[]\n",
        "val_loss_list=[]\n",
        "file_name = 'results'\n",
        "f = open(file_name, 'w+')  # open file in write mode\n",
        "f.write('python rules')\n",
        "f.close()\n",
        "for epoch in tqdm(range(1,epochs+1)):\n",
        "  train_loss=train(train_loader,model,criterion,epoch)\n",
        "  val_loss=evaluate(eval_loader,model,criterion,epoch)\n",
        "  print(\"------Epoch [{}] \\t Train-loss:{:3f} \\t Val-loss{:3f}-----\".format(epoch,train_loss,val_loss))\n",
        "  train_loss_list.append(train_loss)\n",
        "  val_loss_list.append(val_loss)\n",
        "  if epoch==5:\n",
        "    model.embed.embed.requires_grad=True\n",
        "  state={'epoch':epoch,'model':model,'optimizer':transformer_optimizer}\n",
        "  #Test\n",
        "  # if epoch%5==0 and epoch!=0:\n",
        "  #   f1,bleu,meteor=evaluateMetrics(pairs_test)\n",
        "  #   rogue=calculateRogue(pairs_test)\n",
        "    # torch.save(state,'checkpoint-{}.tar'.format(epoch))\n",
        "  "
      ],
      "metadata": {
        "id": "mWvyBC83elD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from matplotlib.pyplot import ylabel\n",
        "# from matplotlib.pylab import plt\n",
        "# from numpy import arange\n",
        "# # Generate a sequence of integers to represent the epoch numbers\n",
        "# y=range(0,10)\n",
        "# # Plot and label the training and validation loss values\n",
        "# plt.plot(y, train_loss_list, label='Training Loss')\n",
        "# plt.plot(y, val_loss_list, label='Validation Loss')\n",
        "\n",
        "# # Add in a title and axes labels\n",
        "# plt.title('Training and Validation Loss')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss')\n",
        "\n",
        "# # Set the tick locations\n",
        "# # plt.xticks(arange(0, epochs))\n",
        "# plt.xticks(arange(0,11,10))\n",
        "\n",
        "# # Display the plot\n",
        "# plt.legend(loc='best')\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "cPL74WiXilah",
        "outputId": "8da86869-ae4a-401d-a6d7-7fa9c4476701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint='checkpoint.tar'\n",
        "if checkpoint:\n",
        "  checkpoint=torch.load(checkpoint)\n",
        "  model=checkpoint['model']"
      ],
      "metadata": {
        "id": "hUjF5B4CfaQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(1):\n",
        "  dialog=input(\"User Input:\")\n",
        "  if dialog=='q':\n",
        "    break\n",
        "  enc_dialog=[vocab.word2idx.get(word,vocab.word2idx['OOV']) for word in dialog.split()]\n",
        "  dialog=torch.LongTensor(enc_dialog).to(device).unsqueeze(0)\n",
        "  dialog_mask=(dialog!=0).to(device).unsqueeze(1).unsqueeze(1)\n",
        "  sentence=evaluateChat(model,dialog,dialog_mask,MAX_LENGTH,vocab)\n",
        "  print(\"Bot: {}\".format(sentence))"
      ],
      "metadata": {
        "id": "W6SfupDcgF-f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "439cd5d7-4d71-4acc-97e2-9594a9013b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Input:नमस्कार कैसे हो तुम?\n",
            "Bot: क्या आप मुझे एक देंगे, द्वारा\n",
            "User Input:चलो कहीं बाहर खाने चलते हैं\n",
            "Bot: क्या आप और कर सकते हैं\n",
            "User Input:मैं आज बहुत खुश हूं\n",
            "Bot: क्या आप मुझे एक और कर सकते हैं\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c4f28c2bb162>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdialog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User Input:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdialog\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0menc_dialog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OOV'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdialog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oaRNK0PKU7p",
        "outputId": "31c33e6c-d65b-4a7e-d61b-a8ee63881342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from rouge import Rouge\n",
        "# def calculateRogue(test_data):\n",
        "#   prediction=[]\n",
        "#   actual=[]\n",
        "#   for pair in test_data:\n",
        "#     dialog=pair[0]\n",
        "#     enc_dialog=[vocab.word2idx.get(word,vocab.word2idx['OOV']) for word in dialog.split()]\n",
        "#     dialog=torch.LongTensor(enc_dialog).to(device).unsqueeze(0)\n",
        "#     dialog_mask=(dialog!=0).to(device).unsqueeze(1).unsqueeze(1)\n",
        "#     pred=evaluateChat(model,dialog,dialog_mask,MAX_LENGTH,vocab)\n",
        "#     if len(pred)<=0:\n",
        "#       continue\n",
        "#     prediction.append(pred)\n",
        "#     actual.append(pair[1])\n",
        "#   rouge = Rouge()\n",
        "#   scores = rouge.get_scores(prediction, actual,avg=True)\n",
        "#   return scores"
      ],
      "metadata": {
        "id": "Y_3utBtf6j63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Bleu Score\n",
        "def bleu_score(guess, answer):\n",
        "    \"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"\n",
        "    bleu1=sentence_bleu([normalizeString(answer).split()],normalizeString(guess).split(\" \"),weights=[1,0,0,0])\n",
        "    bleu2=sentence_bleu([normalizeString(answer).split()],normalizeString(guess).split(\" \"),weights=[0.5,0.5,0,0])\n",
        "    bleu3=sentence_bleu([normalizeString(answer).split()],normalizeString(guess).split(\" \"),weights=[0.3,0.3,0.3,0])\n",
        "    return [bleu1,bleu2,bleu3]"
      ],
      "metadata": {
        "id": "hCRbMNC26pLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##f1_score\n",
        "def prec_recall_f1_score(pred_items, gold_items)->float:\n",
        "    common = Counter(gold_items) & Counter(pred_items)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_items)\n",
        "    recall = 1.0 * num_same / len(gold_items)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def f1_score(guess, answer)-> float:\n",
        "    \"\"\"Return the max F1 score between the guess and *any* answer.\"\"\"\n",
        "    if guess is None or answer is None:\n",
        "        return 0\n",
        "    g_tokens = normalizeString(guess).split()\n",
        "    a_token=normalizeString(answer).split()\n",
        "    return prec_recall_f1_score(g_tokens,a_token )"
      ],
      "metadata": {
        "id": "HzQ52ZaP6rsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Meteor Score\n",
        "def _meteor_score(guess,answer):\n",
        "  return meteor_score([normalizeString(answer).split()],normalizeString(guess).split())"
      ],
      "metadata": {
        "id": "vrGdABUc6uLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateMetrics(test_data):\n",
        "  num_samples=len(test_data)\n",
        "  f1=0.0\n",
        "  bleu=[0.0,0.0,0.0]\n",
        "  meteor=0.0\n",
        "  for pair in test_data:\n",
        "    dialog=pair[0]\n",
        "    actual=pair[1]\n",
        "    enc_dialog=[vocab.word2idx.get(word,vocab.word2idx['OOV']) for word in dialog.split()]\n",
        "    dialog=torch.LongTensor(enc_dialog).to(device).unsqueeze(0)\n",
        "    dialog_mask=(dialog!=0).to(device).unsqueeze(1).unsqueeze(1)\n",
        "    pred=evaluateChat(model,dialog,dialog_mask,MAX_LENGTH,vocab)\n",
        "    f1+=f1_score(pred,actual)\n",
        "    bleu=[a+b for a,b in zip(bleu,bleu_score(pred,actual))]\n",
        "    meteor+=_meteor_score(pred,actual)\n",
        "  f1=(f1/num_samples)*100\n",
        "  bleu[:]=[(x/num_samples)*100 for x in bleu]\n",
        "  meteor=(meteor/num_samples)*100\n",
        "  print(f'F1_score: {f1:.2f}')\n",
        "  print(f'BLEU-1_score: {bleu[0]:.3f}')\n",
        "  print(f'BLEU-2_score: {bleu[1]:.3f}')\n",
        "  print(f'BLEU-3_score: {bleu[2]:.2f}')\n",
        "  print(f'meteor_score: {meteor:.2f}')\n",
        "  return f1,bleu,meteor"
      ],
      "metadata": {
        "id": "X6kpYYXh6xnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,_,_=evaluateMetrics(pairs_test)\n",
        "_,_=calculateRogue(pairs_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liYLlNOeGXzS",
        "outputId": "9d2be87e-5e97-40c4-bb9c-65b0e5a98724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1_score: 11.33\n",
            "BLEU-1_score: 8.487\n",
            "BLEU-2_score: 2.581\n",
            "BLEU-3_score: 1.19\n",
            "meteor_score: 7.29\n",
            "{'rouge-1': {'r': 0.11634827620669214, 'p': 0.14192150054105987, 'f': 0.11713471403360766}, 'rouge-2': {'r': 0.026393339516045382, 'p': 0.03230168130535204, 'f': 0.026538519770754422}, 'rouge-l': {'r': 0.11346450700147566, 'p': 0.1385669623739889, 'f': 0.1142273238885117}}\n"
          ]
        }
      ]
    }
  ]
}